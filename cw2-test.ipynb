{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Your Model\n",
    "The code below illustrates how to load the model trained and saved at notebook *cw2-train.ipynb*, and test the performance of the loaded model with held-out test data. \n",
    "\n",
    "**REMARK 1**: You should adjust the code below according to what components you have saved and how you have saved them.\n",
    "\n",
    "**REMARK 2**: When the markers evaluate your model, they will use the code below (but replace the test data with some held-out data) to run the test. **Hence, make sure you can re-load your model and test its performance with the code below.**\n",
    "\n",
    "**REMARK 3**: If you use embeddings to represent text, **DO NOT** include the embeddings file in your submitted file due to its huge size. Instead, specify which pre-trained embedding you want to use or provide a link for downloading it; the markers will download the embedding and run your code below to load the embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE! The model defined below MUST BE EXACTLY THE SAME as the one you used at training\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvRNN_Classifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_dim, hidden_dim, model_type, cls_num, pooler_type, filter_size_list, filter_num_list, dropout, gpu):\n",
    "        super(ConvRNN_Classifier, self).__init__()\n",
    "        assert model_type in ['rnn','lstm','bilstm','gru']\n",
    "        assert pooler_type in ['max','avg']\n",
    "        \n",
    "        self.embd_dim = embd_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embd_dim)\n",
    "        \n",
    "        # rnn type\n",
    "        if model_type == 'rnn':\n",
    "            self.rnn = nn.RNN(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'lstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, dropout=dropout)\n",
    "        elif model_type == 'bilstm':\n",
    "            self.rnn = nn.LSTM(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, \n",
    "                               bidirectional=True, dropout=dropout)\n",
    "        else: # model_type == 'gru'\n",
    "            self.rnn = nn.GRU(hidden_size=hidden_dim, batch_first=True, input_size=embd_dim, \n",
    "                              dropout=dropout, bidirectional=True, num_layers=2)\n",
    "        \n",
    "        self.convs = self.build_convs(filter_size_list, filter_num_list, gpu)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(np.sum(filter_num_list), 96)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(96, 48)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(48, cls_num)\n",
    "        self.gpu = gpu\n",
    "                \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.gpu = gpu\n",
    "        if gpu: self.to('cuda')\n",
    "            \n",
    "    def build_convs(self, f_sizes, f_nums, gpu):\n",
    "        convs = nn.ModuleList()\n",
    "        for fs, fn in zip(f_sizes, f_nums):\n",
    "            padding_size = fs-1\n",
    "            m = nn.Conv1d(self.hidden_dim, fn, fs, stride=padding_size,padding=padding_size)\n",
    "            if gpu: m.to('cuda')\n",
    "            convs.append(m)\n",
    "        return convs\n",
    "        \n",
    "    def get_conv_output(self, input_matrix, conv, gpu):\n",
    "        # step 1: compute convolution \n",
    "        input_matrix = torch.transpose(input_matrix, 1, 2)\n",
    "#         print('input_mat_conv', input_matrix.shape, 'self.embd_dim', self.embd_dim)\n",
    "        assert input_matrix.shape[1] == self.hidden_dim\n",
    "        if gpu:\n",
    "            input_matrix = input_matrix.to('cuda')\n",
    "        conv_output = conv(input_matrix)\n",
    "        # step 2: pass through an activation function \n",
    "        conv_relu = self.tanh(conv_output)\n",
    "        # step 3: max-over-time pooling\n",
    "        maxp = nn.MaxPool1d(conv_relu.shape[2])\n",
    "        maxp_output = maxp(conv_relu)\n",
    "        return maxp_output\n",
    "    \n",
    "    def forward(self, input_matrix):\n",
    "#         print('ip mat', input_matrix.shape)\n",
    "        token_num = input_matrix.shape[1]\n",
    "#         print('token_num', token_num)\n",
    "        embedding = self.embedding(input_matrix)\n",
    "        hidden_vecs = self.rnn(embedding)[0]\n",
    "#         print('hidden', hidden_vecs.shape)\n",
    "#         print(hidden_vecs)\n",
    "        cnn_repr = torch.tensor([])\n",
    "        if self.gpu: cnn_repr = cnn_repr.to('cuda')\n",
    "        for cv in self.convs:\n",
    "            cv_output = self.get_conv_output(hidden_vecs, cv, self.gpu)\n",
    "            cnn_repr = torch.cat((cnn_repr, cv_output), dim=1)\n",
    "        # print(cnn_repr.shape)\n",
    "        \n",
    "#         cnn_repr = torch.tensor([])\n",
    "#         if self.gpu: cnn_repr = cnn_repr.to('cuda')\n",
    "#         for cv in self.convs:\n",
    "#             cv_output = self.get_conv_output(cnn_repr, cv, self.gpu)\n",
    "#             cnn_repr = torch.cat((cnn_repr, cv_output), dim=1)\n",
    "        \n",
    "        after_dp = self.dropout(cnn_repr.squeeze())\n",
    "        fc1_out = self.relu1(self.fc1(after_dp))\n",
    "        fc2_out = self.relu2(self.fc2(fc1_out))\n",
    "        logit = self.fc3(fc2_out)\n",
    "        # the CrossEntropyLoss provided by pytorch includes softmax; so you do not need to include a softmax layer in your net\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct your trained model from pickle\n",
    "\n",
    "import pickle\n",
    "def reconstruct_model(pickle_path):\n",
    "    model = ConvRNN_Classifier(vocab_size=saved_model_dic['vocab_size'],\n",
    "                               embd_dim=saved_model_dic['input_dim'],\n",
    "                               hidden_dim=saved_model_dic['hidden_dim'],\n",
    "                               model_type=saved_model_dic['model_type'],\n",
    "                               cls_num=saved_model_dic['num_class'],\n",
    "                               pooler_type=saved_model_dic['pooler'],\n",
    "                               filter_size_list=saved_model_dic['filter_sizes'],\n",
    "                               filter_num_list=saved_model_dic['filter_nums'],\n",
    "                               dropout=saved_model_dic['dropout_rate'],\n",
    "                               gpu=saved_model_dic['gpu'])\n",
    "    saved_weights = saved_model_dic['neural_weights']\n",
    "    model.load_state_dict(saved_weights)\n",
    "\n",
    "    word_to_id = saved_model_dic['word_to_id']\n",
    "    max_seq_length = saved_model_dic['max_seq_length']\n",
    "    return model, word_to_id, max_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the reconstructed model to make predictions on the test data\n",
    "\n",
    "import re\n",
    "\n",
    "def word_extract(text):\n",
    "    text = re.sub(r'(-)|(,)|(\\.)', '',text)\n",
    "    text = re.findall(r'[A-Za-z\\d]+[\\w^\\']*', text.lower())\n",
    "    text = [i.strip() for i in text]\n",
    "    return ' '.join(text)\n",
    "\n",
    "def pad_features(data_series, seq_length):\n",
    "\n",
    "    padded_features = np.zeros((len(data_series.values), seq_length), dtype=int)\n",
    "    for i, v in enumerate(data_series):\n",
    "        padded_features[i, -len(v):] = np.array(v)[:seq_length]\n",
    "    return list(padded_features)\n",
    "\n",
    "def word_to_id_converter(word_mappings, data_series):\n",
    "    id_data = []    \n",
    "    for t in data_series:\n",
    "        mapping = []\n",
    "        for i in t.split():\n",
    "            if i in word_mappings.keys():\n",
    "                mapping.append(word_mappings[i])\n",
    "            else:\n",
    "                mapping.append(0)\n",
    "        id_data.append(mapping)\n",
    "    return id_data\n",
    "\n",
    "def make_batch_prediction(batch, model, use_gpu=False):\n",
    "    batch_logits = torch.tensor([])\n",
    "    if use_gpu: \n",
    "        batch_logits = batch_logits.to('cuda')\n",
    "    for i in range(batch.shape[0]):\n",
    "        input_sents = torch.from_numpy(batch[i]).float()\n",
    "        if use_gpu:\n",
    "            input_sents = input_sents.to('cuda')\n",
    "#         print(input_sents.unsqueeze(0).shape)\n",
    "        logits = model(input_sents.unsqueeze(0).long())\n",
    "        batch_logits = torch.cat( (batch_logits, logits) )\n",
    "    return batch_logits.view(batch.shape[0],-1)\n",
    "\n",
    "def test_trained_model(model, test_data, batch_size=32, gpu=False):\n",
    "    with torch.no_grad(): # let pytorch know that no gradient should be computed\n",
    "        model.eval() # let the model know that it in test mode, i.e. no gradient and no dropout\n",
    "        predictions = []\n",
    "        for idx in range(0,len(np.array(test_data)),batch_size):\n",
    "            y_pred = make_batch_prediction(np.array(test_data)[idx:idx+batch_size], model, gpu)\n",
    "            pred_labels = [np.argmax(entry) for entry in y_pred.cpu().detach().numpy()]\n",
    "            predictions += pred_labels\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archie/anaconda3/envs/pt/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data size 2000\n",
      "macro-F1 on test data 0.928540098861077\n"
     ]
    }
   ],
   "source": [
    "# load sample test data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## reconstruct_model\n",
    "model, word_to_id, max_seq_len = reconstruct_model('cw2_crnn_custom_final.pickle')\n",
    "\n",
    "test_data = pd.read_table('../coursework2_train.tsv')\n",
    "test_data['joined_text'] = test_data['article_title'] + ' ' + test_data['sentence_text']\n",
    "test_data['joined_text'] = test_data['joined_text'].apply(word_extract)\n",
    "test_data['joined_text_id'] = word_to_id_converter(word_to_id, test_data['joined_text'].to_list())\n",
    "test_data['joined_text_padded'] = pad_features(test_data['joined_text_id'], max_seq_len)\n",
    "\n",
    "test_text = test_data['joined_text_padded'].tolist()[-2000:]\n",
    "test_raw_labels = test_data['label'].tolist()[-2000:]\n",
    "\n",
    "label_dic = {'non-propaganda':0, 'propaganda':1} \n",
    "\n",
    "test_labels = [label_dic[rl] for rl in test_raw_labels]\n",
    "\n",
    "print('test data size', len(test_labels))\n",
    "\n",
    "test_pred = test_trained_model(model, np.array(test_text), batch_size=32)\n",
    "\n",
    "# test model\n",
    "from sklearn.metrics import precision_recall_fscore_support,accuracy_score\n",
    "pre, rec, f1, _ = precision_recall_fscore_support(test_labels, test_pred, average='macro')\n",
    "print('macro-F1 on test data', f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
